""" Class for Analyzing/Plotting Data Generated by Test Framework """
import os
from yaspin import yaspin
import matplotlib.pyplot as plt
from matplotlib import cm
from pathlib import Path
import numpy as np
import logging
from tabulate import tabulate
class Analyzer:
    def __init__(self,verbose=False):
        self.data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)),'data')
        self.durability_levels = ['durability-low', 'durability-medium', 'durability-high']
        self.operations = ['delete','update','fts','n1qlselect','insert']
        self.bucket_sizes = ['small-bucket', 'medium-bucket', 'large-bucket']
        self.cluster_sizes = [f'cluster-size-{i}' for i in range(1, 6)]
        self.verbose = verbose
        self.setup_logging(verbose=verbose)

    def setup_logging(self, verbose):
        """ set up self.logger for Driver logging """
        self.logger = logging.getLogger('Analyzer')
        formatter = logging.Formatter('%(prefix)s - %(message)s')
        handler = logging.StreamHandler()
        handler.setFormatter(formatter)
        self.prefix = {'prefix': 'Analyzer'}
        self.logger.addHandler(handler)
        self.logger = logging.LoggerAdapter(self.logger, self.prefix )
        if verbose:
            self.logger.setLevel(logging.DEBUG)
            self.logger.debug('Debug mode enabled', extra=self.prefix )
        else:
            self.logger.setLevel(logging.INFO)
    def debug(self, msg):
        self.logger.debug(msg, extra=self.prefix)

    def info(self, msg):
        self.logger.info(msg, extra=self.prefix)

    def error(self, msg):
        self.logger.error(msg, extra=self.prefix)

    def get_cluster_size_folders(self):
        return os.listdir(self.data_dir)

    def get_overall_stats(self):
        """
        Returns {
            'durability-low': {
                'cluster-size-1': {
                    'small-bucket': {
                        {
                            'delete': {
                                'records': [...],
                                'count': N,
                                'min': int
                                'max': int
                                'avg': float
                            },
                            'update': {... },
                            'fts': {... },
                            'n1qlselect': {... },
                            'insert':{ ... }
                        }
                    }
                    'medium-bucket': {
                        ...
                    }
                    'large-bucket': {
                        ...
                    }
                }
            'cluster-size-2': ...
            },
            'durability-medium': {...}

        }
        """
        d = {}
        for durability_level in self.durability_levels:
            d[durability_level] = {}
            for cluster_size in self.cluster_sizes:
                d[durability_level][cluster_size] = {}
                for bucket_size in ['small-bucket', 'medium-bucket', 'large-bucket']:
                    d[durability_level][cluster_size][bucket_size] = {}
                    for operation in self.operations:
                        d[durability_level][cluster_size][bucket_size][operation] = self.get_operation_stats(
                            durability_level=durability_level, cluster_size=cluster_size, bucket_size=bucket_size, operation=operation
                        )
        sorted_d = {}
        for k in sorted(d):
            sorted_d[k] = d[k]
        return sorted_d


    def get_operation_stats(self, durability_level='durability-low', cluster_size='cluster-size-1', bucket_size='small-bucket', operation=''):
        file = os.path.join(
            os.path.dirname(
                os.path.abspath(__file__)),'data', durability_level, cluster_size, bucket_size, operation, 'latencies.txt')
        latencies = []
        with open(file) as f:
            latencies = [float(l) for l in f.readlines()]
        return {
            'records': latencies,
            'count': len(latencies),
            'min': min(latencies),
            'max': max(latencies),
            'avg': sum(latencies) / len(latencies)
        }

    def get_total_operation_stats(self, stats={}, operation=""):
        """ Get the stats (records, min, max, avg) for a specific operation across all durability levels, all cluster sizes, all bucket sizes """
        latencies = []
        for d in self.durability_levels:
            for c in self.cluster_sizes:
                for b in self.bucket_sizes:
                    latencies.extend(stats[d][c][b][operation]['records'])
        return {
            'records': latencies,
            'min': min(latencies),
            'max': max(latencies),
            'avg': sum(latencies) / len(latencies)

        }


    def init_plot_folder(self, name):
        Path(name).mkdir(parents=True, exist_ok=True)

    def generate_box_plots(self, stats={}):
        """ Generate box plots that show latency distribution as related to
        1) durability_level
        2) each cluster size for a given durability level
        3) each bucket size for a given cluster size
        """
        for durability_level in self.durability_levels:
            # Durability Level vs. Operation Latency
            for operation in self.operations:
                plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),
                    'plots','box',durability_level)
                self.init_plot_folder(plot_folder)
                data = self.get_operation_stats_for_durability_level(stats, operation=operation, durability_level=durability_level)
                fig, ax = plt.subplots()
                plt.ylabel('seconds')
                ax.set_title(f'{durability_level},{operation}')
                ax.boxplot(data['records'])
                plt.savefig(os.path.join(plot_folder, f'{operation}.png'))
                plt.close()

            for cluster_size in self.cluster_sizes:
                # Cluster size vs. operation latency (for givne durability level)
                for operation in self.operations:
                    plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),
                        'plots','box',durability_level,cluster_size)
                    self.init_plot_folder(plot_folder)
                    data = self.get_operation_stats_for_cluster_size(
                        stats,operation=operation,durability_level=durability_level,
                        cluster_size=cluster_size)
                    fig, ax = plt.subplots()
                    plt.ylabel('seconds')
                    ax.set_title(f'{durability_level},{cluster_size},{operation}')
                    ax.boxplot(data['records'])
                    plt.savefig(os.path.join(plot_folder, f'{operation}.png'))
                    plt.close()

                for bucket_size in self.bucket_sizes:
                    for operation in self.operations:
                        plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),
                            'plots','box',durability_level,cluster_size, bucket_size)
                        self.init_plot_folder(plot_folder)
                        data = self.get_operation_stats_for_bucket_size(
                            stats, operation=operation, durability_level=durability_level,
                            cluster_size=cluster_size,bucket_size=bucket_size
                        )
                        fig, ax = plt.subplots()
                        plt.ylabel('seconds')
                        ax.set_title(f'{durability_level},{cluster_size},{bucket_size},{operation}')
                        ax.boxplot(data['records'])
                        plt.savefig(os.path.join(plot_folder, f'{operation}.png'))
                        plt.close()

    def get_operation_stats_for_durability_level(self, stats, operation="",durability_level=""):
        """ Get all operation latency stats for a given durability level """
        data = stats[durability_level]
        latencies = []
        for cluster_size in data.keys():
            cluster_data = data[cluster_size]
            for bucket_size in cluster_data.keys():
                bucket_data = cluster_data[bucket_size]
                for op,op_stats in bucket_data.items():
                    if op == operation:
                        latencies.extend(op_stats['records'])
        return {
            'records': latencies,
            'avg': sum(latencies) / len(latencies),
            'max': max(latencies),
            'min': min(latencies)
        }

    def get_operation_stats_for_cluster_size(self, stats, operation="", durability_level="", cluster_size=""):
        """ Get the latency data for an operation across an entire cluster size (within a given durability level), not bucket-size specific """
        data = stats[durability_level][cluster_size]
        latencies = []
        for bucket_size in data.keys():
            for op,op_stats in data[bucket_size].items():
                if op == operation:
                    latencies.extend(op_stats['records'])
        return {
            'records': latencies,
            'avg': sum(latencies) / len(latencies),
            'max': max(latencies),
            'min': min(latencies)
        }

    def get_operation_stats_for_bucket_size(self, stats, operation="", durability_level="", cluster_size="", bucket_size=""):
        """ Get the latency data for an operation across a bucket size (in a given
        durability level and a given cluster size) """
        return stats[durability_level][cluster_size][bucket_size][operation]

    def generate_line_graphs_cluster_size_v_latency(self, stats={}, operation=""):
        """ Generate line graph that reveals relationship between cluster size and operation latency for  operation """
        # Cluster sizes on x axis
        x = np.array(self.cluster_sizes)
        fig, ax = plt.subplots()
        for durability_level in self.durability_levels:
            _mins, _maxes, _avgs = [], [], []

            plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','line',durability_level)
            self.init_plot_folder(plot_folder)
            for cluster_size in self.cluster_sizes:
                # Cluster size v. operation latency
                title = f'cluster size vs. {operation} latency'
                data = self.get_operation_stats_for_cluster_size(
                    stats,operation=operation,durability_level=durability_level,
                    cluster_size=cluster_size
                )
                _mins.append(data['min'])
                _maxes.append(data['max'])
                _avgs.append(data['avg'])
            ax.set_title(title)
            plt.ylabel('seconds')
            _mins = np.array(_mins)
            _maxes = np.array(_maxes)
            _avgs = np.array(_avgs)
            plt.plot(x, _mins, label="minimum", linestyle="--")
            plt.plot(x, _maxes, label="maximum", linestyle="-.")
            plt.plot(x, _avgs, label="average", linestyle="-")
            plt.legend()
            plt.savefig(os.path.join(plot_folder, f'cluster-size-v-{operation}.png'))
            plt.close()

    def generate_line_graphs_bucket_size_v_latency(self, stats={}, operation=""):
        """ Generate line graph that reveals relationship between bucket size
        and operation latency for each cluster size within each durability level """
        # Cluster sizes on x axis
        x = np.array(self.bucket_sizes)
        fig, ax = plt.subplots()
        for durability_level in self.durability_levels:
            for cluster_size in self.cluster_sizes:
                plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','line',durability_level, cluster_size)
                self.init_plot_folder(plot_folder)
                _mins = []
                _maxes = []
                _avgs = []
                for bucket_size in self.bucket_sizes:
                    title = f'{durability_level},{cluster_size}, bucket size vs. {operation} latency'
                    data = self.get_operation_stats_for_bucket_size(
                        stats, operation=operation,durability_level=durability_level,
                        cluster_size=cluster_size,bucket_size=bucket_size
                    )
                    _mins.append(data['min'])
                    _maxes.append(data['max'])
                    _avgs.append(data['avg'])
                ax.set_title(title)
                plt.ylabel('seconds')
                _mins = np.array(_mins)
                _maxes = np.array(_maxes)
                _avgs = np.array(_avgs)
                plt.plot(x, _mins, label="minimum", linestyle="--")
                plt.plot(x, _maxes, label="maximum", linestyle="-.")
                plt.plot(x, _avgs, label="average", linestyle="-")
                plt.legend()
                plt.savefig(os.path.join(plot_folder, f'bucket-size-vs-{operation}.png'))
                plt.close()

    def generate_line_graph_durability_v_latency(self, stats={}, operation=""):
        """ Generate a single multiline graph, one line per operation, showing relationship
        between durability tuning and average latency """
        x = np.array(self.durability_levels)
        fig,ax = plt.subplots()
        ax.set_title('durability level vs operation latency')
        plt.ylabel('seconds')
        plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','line')
        self.init_plot_folder(plot_folder)
        for operation in self.operations:
            # Each op = one line
            _avgs = []
            for durability_level in self.durability_levels:
                data = self.get_operation_stats_for_durability_level(stats,
                    operation=operation,durability_level=durability_level)
                _avgs.append(data['avg'])
            _avgs = np.array(_avgs)
            plt.plot(x, _avgs, label=f'{operation} avg latency', linestyle="-.")
        plt.legend()
        plt.savefig(os.path.join(plot_folder, f'durability-vs-operations.png'))
        plt.close()

    def generate_3d_scatterplot(self, stats={}):
        x = self.durability_levels
        y = self.cluster_sizes

        # Generate one 3d plot per operation
        plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','3dscatter')
        self.init_plot_folder(plot_folder)
        DURABILITY_MAP = {
            'durability-low': 0,
            'durability-medium' : 1,
            'durability-high': 2
        }
        def CLUSTER_SIZE_MAP(cluster_size_string=""):
            return int(cluster_size_string.split('-')[-1])

        for operation in self.operations:
            fig = plt.figure()
            ax = fig.add_subplot(111,projection='3d')
            X = []
            Y = []
            Z = []
            for d in self.durability_levels:
                for c in self.cluster_sizes:
                        # Plot the average latency for this operation at each combination of durability level, cluster size, and bucket size.
                        # Each combo = one point on 3d plot.
                        data = self.get_operation_stats_for_cluster_size(
                            stats,operation=operation,durability_level=d, cluster_size=c
                        )
                        X.append(DURABILITY_MAP[d])
                        Y.append(CLUSTER_SIZE_MAP(c))
                        Z.append(data['avg'])

            ax.scatter(X,Y,Z,marker='o')
            durability_ticks = np.arange(0,3,1)
            ax.set_xticks(durability_ticks)
            ax.set_title(f'cluster size & durability impact on {operation} latency')
            ax.set_xlabel('durability level')
            ax.set_ylabel('cluster size')
            ax.set_zlabel('  latency (sec)')
            ax.view_init(elev=20)
            plt.savefig(os.path.join(plot_folder, f'{operation}.png'))
            plt.close()

    def build_table_durability_mutation_ops_cluster_size_5(self, stats={}):
        """ Build a table with rows for operations and columns for durability levels with
        the cells as the average latencies
        for those operations at those durability configs for cluster size 5 """
        rows = []
        rows.append(
            ['','','Durability Level', '']
        )
        rows.append(
            ['Operation','low','medium','high']
        )
        for operation in self.operations:
            # Build a row for this operation
            rows.append(
                [operation,
                self.get_operation_stats_for_cluster_size(stats,operation=operation,durability_level='durability-low',cluster_size='cluster-size-5')['avg'],
                self.get_operation_stats_for_cluster_size(stats,operation=operation,durability_level='durability-medium',cluster_size='cluster-size-5')['avg'],
                self.get_operation_stats_for_cluster_size(stats,operation=operation,durability_level='durability-high',cluster_size='cluster-size-5')['avg'],
                ]
            )
        table = tabulate(rows)
        plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','tables')
        self.init_plot_folder(plot_folder)
        with open(os.path.join(plot_folder, f'durability.txt'), 'w') as f:
            f.write(table)

    def build_table_cluster_size_v_operation_latency_low_durability(self, stats={}):
        """ Build a table with rows for each operation and columns for
        cluster sizes with the cells as the average latencies
        for those operations in those cluster sizes (with durability=low for all) """
        rows = []
        rows.append(
            ['','','', 'Cluster Size','','']
        )
        rows.append(
            ['Operation','1','2', '3', '4', '5']
        )
        for operation in self.operations:
            # Build a row for this operation
            rows.append(
                [operation,
                self.get_operation_stats_for_cluster_size(stats,operation=operation,durability_level='durability-low',cluster_size='cluster-size-1')['avg'],
                self.get_operation_stats_for_cluster_size(stats,operation=operation,durability_level='durability-low',cluster_size='cluster-size-2')['avg'],
                self.get_operation_stats_for_cluster_size(stats,operation=operation,durability_level='durability-low',cluster_size='cluster-size-3')['avg'],
                self.get_operation_stats_for_cluster_size(stats,operation=operation,durability_level='durability-low',cluster_size='cluster-size-4')['avg'],
                self.get_operation_stats_for_cluster_size(stats,operation=operation,durability_level='durability-low',cluster_size='cluster-size-5')['avg']
                ]
            )
        table = tabulate(rows)
        plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','tables')
        self.init_plot_folder(plot_folder)
        with open(os.path.join(plot_folder, f'clustersize.txt'), 'w') as f:
            f.write(table)

    def build_table_bucket_size_v_operation_latency_low_durability(self, stats={}):
        """ Build a table with rows for operations, columns as bucket sizes, using durability=low, cluster_size=5;
        each cell is average latency at that intersection """
        rows = []
        rows.append(
            ['','', 'Bucket Size','']
        )
        rows.append(
            ['Operation','small(n=1000)','medium(n=3000)', 'large(n=5000)']
        )
        for operation in self.operations:
            # Build a row for this operation
            rows.append(
                [operation,
                self.get_operation_stats_for_bucket_size(stats,operation=operation,durability_level='durability-low', cluster_size='cluster-size-5',bucket_size='small-bucket')['avg'],
                self.get_operation_stats_for_bucket_size(stats,operation=operation,durability_level='durability-low', cluster_size='cluster-size-5',bucket_size='medium-bucket')['avg'],
                self.get_operation_stats_for_bucket_size(stats,operation=operation,durability_level='durability-low', cluster_size='cluster-size-5',bucket_size='large-bucket')['avg'],
                ]
            )
        table = tabulate(rows)
        plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','tables')
        self.init_plot_folder(plot_folder)
        with open(os.path.join(plot_folder, f'bucketsize.txt'), 'w') as f:
            f.write(table)

    def build_table_operation_type_v_latency(self, stats={}):
        """ Build a table with rows as [min,max,avg] and a column for each operation type
        with durability = all, cluster size = all, bucket size = all """
        rows = []
        rows.append(
            ['','', '','Operation','','']
        )
        rows.append(
            ['Measure','delete','update','fts','n1qlselect','insert']
        )
        for measure in ['min','max','avg']:
            # One row per measure
            # Build a row with a column for each operation
            ops_columns = [
                self.get_total_operation_stats(stats,operation=op)[measure] for op in self.operations
            ]
            rows.append(
                [measure] + ops_columns
            )
        table = tabulate(rows)
        plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','tables')
        self.init_plot_folder(plot_folder)
        with open(os.path.join(plot_folder, f'operation_type.txt'), 'w') as f:
            f.write(table)

    def plot(self):
        stats = self.get_overall_stats()
        self.info("Generating plots...")
        with yaspin().white.bold.shark.on_blue as sp:
            self.generate_box_plots(stats)
            self.generate_3d_scatterplot(stats)
            self.build_table_durability_mutation_ops_cluster_size_5(stats)
            self.build_table_cluster_size_v_operation_latency_low_durability(stats)
            self.build_table_bucket_size_v_operation_latency_low_durability(stats)
            self.build_table_operation_type_v_latency(stats)
            for op in self.operations:
                self.generate_line_graphs_cluster_size_v_latency(stats, operation=op)
                self.generate_line_graphs_bucket_size_v_latency(stats, operation=op)
                self.generate_line_graph_durability_v_latency(stats,operation=op)
