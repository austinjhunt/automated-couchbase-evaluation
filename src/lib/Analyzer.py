""" Class for Analyzing/Plotting Data Generated by Test Framework """
import os
from yaspin import yaspin
import matplotlib.pyplot as plt
from matplotlib import cm
from pathlib import Path
import numpy as np
import logging
import sys
import json
from collections import OrderedDict
from tabulate import tabulate
class Analyzer:
    def __init__(self,verbose=False):
        self.data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)),'data')
        self.durability_levels = ['durability-low', 'durability-medium', 'durability-high']
        self.operations = ['delete','update','fts','n1qlselect','insert']
        self.bucket_sizes = ['small-bucket', 'medium-bucket', 'large-bucket']
        self.cluster_sizes = [f'cluster-size-{i}' for i in range(1, 6)]
        self.verbose = verbose
        self.setup_logging(verbose=verbose)

    def setup_logging(self, verbose):
        """ set up self.logger for Driver logging """
        self.logger = logging.getLogger('Analyzer')
        formatter = logging.Formatter('%(prefix)s - %(message)s')
        handler = logging.StreamHandler()
        handler.setFormatter(formatter)
        self.prefix = {'prefix': 'Analyzer'}
        self.logger.addHandler(handler)
        self.logger = logging.LoggerAdapter(self.logger, self.prefix )
        if verbose:
            self.logger.setLevel(logging.DEBUG)
            self.logger.debug('Debug mode enabled', extra=self.prefix )
        else:
            self.logger.setLevel(logging.INFO)

    def debug(self, msg):
        self.logger.debug(msg, extra=self.prefix)

    def info(self, msg):
        self.logger.info(msg, extra=self.prefix)

    def error(self, msg):
        self.logger.error(msg, extra=self.prefix)

    def get_cluster_size_folders(self):
        return os.listdir(self.data_dir)

    def get_overall_stats(self):
        """
        Returns {
            'durability-low': {
                'cluster-size-1': {
                    'small-bucket': {
                        {
                            'delete': {
                                'records': [...],
                                'count': N,
                                'min': int
                                'max': int
                                'avg': float
                            },
                            'update': {... },
                            'fts': {... },
                            'n1qlselect': {... },
                            'insert':{ ... }
                        }
                    }
                    'medium-bucket': {
                        ...
                    }
                    'large-bucket': {
                        ...
                    }
                }
            'cluster-size-2': ...
            },
            'durability-medium': {...}

        }
        """
        d = {}
        for durability_level in self.durability_levels:
            d[durability_level] = {}
            for cluster_size in self.cluster_sizes:
                d[durability_level][cluster_size] = {}
                for bucket_size in ['small-bucket', 'medium-bucket', 'large-bucket']:
                    d[durability_level][cluster_size][bucket_size] = {}
                    for operation in self.operations:
                        d[durability_level][cluster_size][bucket_size][operation] = self.get_operation_stats(
                            durability_level=durability_level, cluster_size=cluster_size, bucket_size=bucket_size, operation=operation
                        )
        sorted_d = {}
        for k in sorted(d):
            sorted_d[k] = d[k]
        return sorted_d


    def get_operation_stats(self, durability_level='durability-low', cluster_size='cluster-size-1', bucket_size='small-bucket', operation=''):
        file = os.path.join(
            os.path.dirname(
                os.path.abspath(__file__)),'data', durability_level, cluster_size, bucket_size, operation, 'latencies.txt')
        latencies = []
        with open(file) as f:
            latencies = [float(l) for l in f.readlines()]
        return {
            'records': latencies,
            'count': len(latencies),
            'min': min(latencies),
            'max': max(latencies),
            'avg': sum(latencies) / len(latencies)
        }

    def get_total_operation_stats(self, stats={}, operation=""):
        """ Get the stats (records, min, max, avg) for a specific operation across all durability levels, all cluster sizes, all bucket sizes """
        latencies = []
        for d in self.durability_levels:
            for c in self.cluster_sizes:
                for b in self.bucket_sizes:
                    latencies.extend(stats[d][c][b][operation]['records'])
        return {
            'records': latencies,
            'min': min(latencies),
            'max': max(latencies),
            'avg': sum(latencies) / len(latencies)

        }


    def init_plot_folder(self, name):
        Path(name).mkdir(parents=True, exist_ok=True)

    def generate_box_plots(self, stats={}):
        """ Generate box plots that show latency distribution as related to
        1) durability_level
        2) each cluster size for a given durability level
        3) each bucket size for a given cluster size
        """
        for durability_level in self.durability_levels:
            # Durability Level vs. Operation Latency
            for operation in self.operations:
                plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),
                    'plots','box',durability_level)
                self.init_plot_folder(plot_folder)
                data = self.get_operation_stats_for_durability_level(stats, operation=operation, durability_level=durability_level)
                fig, ax = plt.subplots()
                plt.ylabel('seconds')
                ax.set_title(f'{durability_level},{operation}')
                ax.boxplot(data['records'])
                plt.savefig(os.path.join(plot_folder, f'{operation}.png'))
                plt.close()

            for cluster_size in self.cluster_sizes:
                # Cluster size vs. operation latency (for givne durability level)
                for operation in self.operations:
                    plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),
                        'plots','box',durability_level,cluster_size)
                    self.init_plot_folder(plot_folder)
                    data = self.get_operation_stats_for_cluster_size(
                        stats,operation=operation,durability_level=durability_level,
                        cluster_size=cluster_size)
                    fig, ax = plt.subplots()
                    plt.ylabel('seconds')
                    ax.set_title(f'{durability_level},{cluster_size},{operation}')
                    ax.boxplot(data['records'])
                    plt.savefig(os.path.join(plot_folder, f'{operation}.png'))
                    plt.close()

                for bucket_size in self.bucket_sizes:
                    for operation in self.operations:
                        plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),
                            'plots','box',durability_level,cluster_size, bucket_size)
                        self.init_plot_folder(plot_folder)
                        data = self.get_operation_stats_for_bucket_size(
                            stats, operation=operation, durability_level=durability_level,
                            cluster_size=cluster_size,bucket_size=bucket_size
                        )
                        fig, ax = plt.subplots()
                        plt.ylabel('seconds')
                        ax.set_title(f'{durability_level},{cluster_size},{bucket_size},{operation}')
                        ax.boxplot(data['records'])
                        plt.savefig(os.path.join(plot_folder, f'{operation}.png'))
                        plt.close()

    def get_operation_stats_for_durability_level(self, stats, operation="",durability_level=""):
        """ Get all operation latency stats for a given durability level """
        data = stats[durability_level]
        latencies = []
        for cluster_size in data.keys():
            cluster_data = data[cluster_size]
            for bucket_size in cluster_data.keys():
                bucket_data = cluster_data[bucket_size]
                for op,op_stats in bucket_data.items():
                    if op == operation:
                        latencies.extend(op_stats['records'])
        return {
            'records': latencies,
            'avg': sum(latencies) / len(latencies),
            'max': max(latencies),
            'min': min(latencies)
        }

    def get_operation_stats_for_cluster_size(self, stats, operation="", durability_level="", cluster_size=""):
        """ Get the latency data for an operation across an entire cluster size (within a given durability level), not bucket-size specific """
        data = stats[durability_level][cluster_size]
        latencies = []
        for bucket_size in data.keys():
            for op,op_stats in data[bucket_size].items():
                if op == operation:
                    latencies.extend(op_stats['records'])
        return {
            'records': latencies,
            'avg': sum(latencies) / len(latencies),
            'max': max(latencies),
            'min': min(latencies)
        }

    def get_operation_stats_for_bucket_size(self, stats, operation="", durability_level="", cluster_size="", bucket_size=""):
        """ Get the latency data for an operation across a bucket size (in a given
        durability level and a given cluster size) """
        return stats[durability_level][cluster_size][bucket_size][operation]

    def generate_line_graphs_cluster_size_v_latency(self, stats={}, operation=""):
        """ Generate line graph that reveals relationship between cluster size and operation latency for  operation """
        # Cluster sizes on x axis
        x = np.array(self.cluster_sizes)
        fig, ax = plt.subplots()
        for durability_level in self.durability_levels:
            _mins, _maxes, _avgs = [], [], []

            plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','line',durability_level)
            self.init_plot_folder(plot_folder)
            for cluster_size in self.cluster_sizes:
                # Cluster size v. operation latency
                title = f'cluster size vs. {operation} latency'
                data = self.get_operation_stats_for_cluster_size(
                    stats,operation=operation,durability_level=durability_level,
                    cluster_size=cluster_size
                )
                _mins.append(data['min'])
                _maxes.append(data['max'])
                _avgs.append(data['avg'])
            ax.set_title(title)
            plt.ylabel('seconds')
            _mins = np.array(_mins)
            _maxes = np.array(_maxes)
            _avgs = np.array(_avgs)
            plt.plot(x, _mins, label="minimum", linestyle="--")
            plt.plot(x, _maxes, label="maximum", linestyle="-.")
            plt.plot(x, _avgs, label="average", linestyle="-")
            plt.legend()
            plt.savefig(os.path.join(plot_folder, f'cluster-size-v-{operation}.png'))
            plt.close()

    def generate_line_graphs_bucket_size_v_latency(self, stats={}, operation=""):
        """ Generate line graph that reveals relationship between bucket size
        and operation latency for each cluster size within each durability level """
        # Cluster sizes on x axis
        x = np.array(self.bucket_sizes)
        fig, ax = plt.subplots()
        for durability_level in self.durability_levels:
            for cluster_size in self.cluster_sizes:
                plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','line',durability_level, cluster_size)
                self.init_plot_folder(plot_folder)
                _mins = []
                _maxes = []
                _avgs = []
                for bucket_size in self.bucket_sizes:
                    title = f'{durability_level},{cluster_size}, bucket size vs. {operation} latency'
                    data = self.get_operation_stats_for_bucket_size(
                        stats, operation=operation,durability_level=durability_level,
                        cluster_size=cluster_size,bucket_size=bucket_size
                    )
                    _mins.append(data['min'])
                    _maxes.append(data['max'])
                    _avgs.append(data['avg'])
                ax.set_title(title)
                plt.ylabel('seconds')
                _mins = np.array(_mins)
                _maxes = np.array(_maxes)
                _avgs = np.array(_avgs)
                plt.plot(x, _mins, label="minimum", linestyle="--")
                plt.plot(x, _maxes, label="maximum", linestyle="-.")
                plt.plot(x, _avgs, label="average", linestyle="-")
                plt.legend()
                plt.savefig(os.path.join(plot_folder, f'bucket-size-vs-{operation}.png'))
                plt.close()

    def generate_line_graph_durability_v_latency(self, stats={}, operation=""):
        """ Generate a single multiline graph, one line per operation, showing relationship
        between durability tuning and average latency """
        x = np.array(self.durability_levels)
        fig,ax = plt.subplots()
        ax.set_title('durability level vs operation latency')
        plt.ylabel('seconds')
        plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','line')
        self.init_plot_folder(plot_folder)
        for operation in self.operations:
            # Each op = one line
            _avgs = []
            for durability_level in self.durability_levels:
                data = self.get_operation_stats_for_durability_level(stats,
                    operation=operation,durability_level=durability_level)
                _avgs.append(data['avg'])
            _avgs = np.array(_avgs)
            plt.plot(x, _avgs, label=f'{operation} avg latency', linestyle="-.")
        plt.legend()
        plt.savefig(os.path.join(plot_folder, f'durability-vs-operations.png'))
        plt.close()

    def generate_3d_scatterplot(self, stats={}):
        x = self.durability_levels
        y = self.cluster_sizes

        # Generate one 3d plot per operation
        plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','3dscatter')
        self.init_plot_folder(plot_folder)
        DURABILITY_MAP = {
            'durability-low': 0,
            'durability-medium' : 1,
            'durability-high': 2
        }
        def CLUSTER_SIZE_MAP(cluster_size_string=""):
            return int(cluster_size_string.split('-')[-1])

        for operation in self.operations:
            fig = plt.figure()
            ax = fig.add_subplot(111,projection='3d')
            X = []
            Y = []
            Z = []
            for d in self.durability_levels:
                for c in self.cluster_sizes:
                        # Plot the average latency for this operation at each combination of durability level, cluster size, and bucket size.
                        # Each combo = one point on 3d plot.
                        data = self.get_operation_stats_for_cluster_size(
                            stats,operation=operation,durability_level=d, cluster_size=c
                        )
                        X.append(DURABILITY_MAP[d])
                        Y.append(CLUSTER_SIZE_MAP(c))
                        Z.append(data['avg'])

            ax.scatter(X,Y,Z,marker='o')
            durability_ticks = np.arange(0,3,1)
            ax.set_xticks(durability_ticks)
            ax.set_title(f'cluster size & durability impact on {operation} latency')
            ax.set_xlabel('durability level')
            ax.set_ylabel('cluster size')
            ax.set_zlabel('  latency (sec)')
            ax.view_init(elev=20)
            plt.savefig(os.path.join(plot_folder, f'{operation}.png'))
            plt.close()

    def build_table_durability_mutation_ops_cluster_size_5(self, stats={}):
        """ Build a table with rows for operations and columns for durability levels with
        the cells as the average latencies
        for those operations at those durability configs for cluster size 5 """
        rows = []
        rows.append(
            ['','','Durability Level', '']
        )
        rows.append(
            ['Operation','low','medium','high']
        )
        for operation in self.operations:
            # Build a row for this operation
            rows.append(
                [operation,
                self.get_operation_stats_for_cluster_size(stats,operation=operation,durability_level='durability-low',cluster_size='cluster-size-5')['avg'],
                self.get_operation_stats_for_cluster_size(stats,operation=operation,durability_level='durability-medium',cluster_size='cluster-size-5')['avg'],
                self.get_operation_stats_for_cluster_size(stats,operation=operation,durability_level='durability-high',cluster_size='cluster-size-5')['avg'],
                ]
            )
        table = tabulate(rows)
        plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','tables')
        self.init_plot_folder(plot_folder)
        with open(os.path.join(plot_folder, f'durability.txt'), 'w') as f:
            f.write(table)

    def build_table_cluster_size_v_operation_latency_low_durability(self, stats={}):
        """ Build a table with rows for each operation and columns for
        cluster sizes with the cells as the average latencies
        for those operations in those cluster sizes (with durability=low for all) """
        rows = []
        rows.append(
            ['','','', 'Cluster Size','','']
        )
        rows.append(
            ['Operation','1','2', '3', '4', '5']
        )
        for operation in self.operations:
            # Build a row for this operation
            rows.append(
                [operation,
                self.get_operation_stats_for_cluster_size(stats,operation=operation,durability_level='durability-low',cluster_size='cluster-size-1')['avg'],
                self.get_operation_stats_for_cluster_size(stats,operation=operation,durability_level='durability-low',cluster_size='cluster-size-2')['avg'],
                self.get_operation_stats_for_cluster_size(stats,operation=operation,durability_level='durability-low',cluster_size='cluster-size-3')['avg'],
                self.get_operation_stats_for_cluster_size(stats,operation=operation,durability_level='durability-low',cluster_size='cluster-size-4')['avg'],
                self.get_operation_stats_for_cluster_size(stats,operation=operation,durability_level='durability-low',cluster_size='cluster-size-5')['avg']
                ]
            )
        table = tabulate(rows)
        plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','tables')
        self.init_plot_folder(plot_folder)
        with open(os.path.join(plot_folder, f'clustersize.txt'), 'w') as f:
            f.write(table)

    def build_table_bucket_size_v_operation_latency_low_durability(self, stats={}):
        """ Build a table with rows for operations, columns as bucket sizes, using durability=low, cluster_size=5;
        each cell is average latency at that intersection """
        rows = []
        rows.append(
            ['','', 'Bucket Size','']
        )
        rows.append(
            ['Operation','small(n=1000)','medium(n=3000)', 'large(n=5000)']
        )
        for operation in self.operations:
            # Build a row for this operation
            rows.append(
                [operation,
                self.get_operation_stats_for_bucket_size(stats,operation=operation,durability_level='durability-low', cluster_size='cluster-size-5',bucket_size='small-bucket')['avg'],
                self.get_operation_stats_for_bucket_size(stats,operation=operation,durability_level='durability-low', cluster_size='cluster-size-5',bucket_size='medium-bucket')['avg'],
                self.get_operation_stats_for_bucket_size(stats,operation=operation,durability_level='durability-low', cluster_size='cluster-size-5',bucket_size='large-bucket')['avg'],
                ]
            )
        table = tabulate(rows)
        plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','tables')
        self.init_plot_folder(plot_folder)
        with open(os.path.join(plot_folder, f'bucketsize.txt'), 'w') as f:
            f.write(table)

    def build_table_operation_type_v_latency(self, stats={}):
        """ Build a table with rows as [min,max,avg] and a column for each operation type
        with durability = all, cluster size = all, bucket size = all """
        rows = []
        rows.append(
            ['','', '','Operation','','']
        )
        rows.append(
            ['Measure','delete','update','fts','n1qlselect','insert']
        )
        for measure in ['min','max','avg']:
            # One row per measure
            # Build a row with a column for each operation
            ops_columns = [
                self.get_total_operation_stats(stats,operation=op)[measure] for op in self.operations
            ]
            rows.append(
                [measure] + ops_columns
            )
        table = tabulate(rows)
        plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','tables')
        self.init_plot_folder(plot_folder)
        with open(os.path.join(plot_folder, f'operation_type.txt'), 'w') as f:
            f.write(table)

    def plot(self):
        stats = self.get_overall_stats()
        self.info("Generating plots...")
        with yaspin().white.bold.shark.on_blue as sp:
            self.generate_box_plots(stats)
            self.generate_3d_scatterplot(stats)
            self.build_table_durability_mutation_ops_cluster_size_5(stats)
            self.build_table_cluster_size_v_operation_latency_low_durability(stats)
            self.build_table_bucket_size_v_operation_latency_low_durability(stats)
            self.build_table_operation_type_v_latency(stats)
            for op in self.operations:
                self.generate_line_graphs_cluster_size_v_latency(stats, operation=op)
                self.generate_line_graphs_bucket_size_v_latency(stats, operation=op)
                self.generate_line_graph_durability_v_latency(stats,operation=op)

    def file_lines_that_contain(self, string, fname):
        try:
            with open(fname, 'r') as fp:
                return [line for line in fp if string in line]
        except:
            return []

    def collect_ycsb_stats_to_json(self):
        """ Use the Raw measurement data from YCSB to generate a
        dictionary of aggregated data """
        ycsb_data_folder = os.path.join(os.path.dirname(
            os.path.abspath(__file__)),'data','ycsb-results')


        latency_by_record_count = {}
        latency_by_field_count = {}
        latency_by_field_length = {}
        latency_by_request_distribution = {}

        for data_file in os.listdir(ycsb_data_folder):
            file_pointer = os.path.join(
                os.path.dirname(os.path.abspath(__file__)),
                'data','ycsb-results', data_file)
            # data file name format:
            # csz<CLUSTER_SIZE>-rc<RECORD_COUNT>-fc<FIELD_COUNT>-fl<FIELD_LENGTH_BYTES>-
            # rd<REQUEST_DISTRIBUTION>-r<READ_PROPORTION>-u<UPDATE_PROPORTION>-
            # s<SCAN_PROPORTION>-i<INSERT_PROPORTION>
            param_value_pairs = data_file.split('-')
            cluster_size = param_value_pairs[0].split('csz')[-1]

            # initialize each key's value to empty dict, format will be
            # Key = READPROPORTION-UPDATEPROPORTION-INSERTPROPORTION
            # Value = {
            #   'read':  {percentile:[latency_list...], percentile: [latency_list]}
            #   'update':  {percentile:[latency_list...], percentile: [latency_list]}
            #   'insert': {percentile:[latency_list...], percentile: [latency_list]}
            # }

            # Data set size
            record_count = int(param_value_pairs[1].split('rc')[-1])
            if record_count not in latency_by_record_count:
                 # How does tail latency of each operation type change with record count?
                latency_by_record_count[record_count] = {}

            field_count = int(param_value_pairs[2].split('fc')[-1])
            if field_count not in latency_by_field_count:
                # How does tail latency of each operation type change with field count?
                latency_by_field_count[field_count] = {}

            field_length = int(param_value_pairs[3].split('fl')[-1])
            if field_length not in latency_by_field_length:
                # How does tail latency of each operation type change with field length?
                latency_by_field_length[field_length] = {}

            # Request distribution
            request_distribution = param_value_pairs[4].split('rd')[-1]
            if request_distribution not in latency_by_request_distribution:
                # How does tail latency of each operation type change with request distribution?
                latency_by_request_distribution[request_distribution] = {}

            # Operation Proportions
            read_proportion = float(param_value_pairs[5].split('r')[-1])
            update_proportion = float(param_value_pairs[6].split('u')[-1])
            insert_proportion = float(param_value_pairs[8].split('i')[-1].replace('.data',''))
            op_proportions_key = f'{read_proportion}-{update_proportion}-{insert_proportion}'

            init_percentiles_dict = {'90':[], '95':[], '99':[], '99.9':[], '99.99':[]}
            init_op_latencies_for_current_proportions = {
                    'read': init_percentiles_dict,
                    'insert': init_percentiles_dict,
                    'update': init_percentiles_dict
                }

            if op_proportions_key not in latency_by_record_count[record_count]:
                latency_by_record_count[record_count][op_proportions_key] = init_op_latencies_for_current_proportions

            if op_proportions_key not in latency_by_field_length[field_length]:
                latency_by_field_length[field_length][op_proportions_key] = init_op_latencies_for_current_proportions

            if op_proportions_key not in latency_by_field_count[field_count]:
                latency_by_field_count[field_count][op_proportions_key] = init_op_latencies_for_current_proportions

            if op_proportions_key not in latency_by_request_distribution[request_distribution]:
                latency_by_request_distribution[request_distribution][op_proportions_key] = init_op_latencies_for_current_proportions

            for percentile in ['90', '95', '99', '99.9', '99.99']:
                # We should only be interested in this latency information if
                #
                try:
                    read_microseconds_tail_latency = int(self.file_lines_that_contain(
                        f'[READ], p{percentile}',
                        file_pointer
                    )[0].split(', ')[-1])
                except Exception as e:
                    self.error(f'{e}, setting read_microseconds_tail_latency = None ')
                    # did not execute any read operations in this file
                    read_microseconds_tail_latency = None
                try:
                    insert_microseconds_tail_latency = int(self.file_lines_that_contain(
                        f'[INSERT], p{percentile}',
                        file_pointer
                    )[0].split(', ')[-1])
                except Exception as e:
                    self.error(f'{e}, setting insert_microseconds_tail_latency = None ')
                    # did not execute any insert operations in this file
                    insert_microseconds_tail_latency = None
                try:
                    update_microseconds_tail_latency = int(self.file_lines_that_contain(
                        f'[UPDATE], p{percentile}',
                        file_pointer
                    )[0].split(', ')[-1])
                except Exception as e:
                    self.error(f'{e}, setting update_microseconds_tail_latency = None ')
                    # did not execute any update operations in this file
                    update_microseconds_tail_latency = None

                for op, latency in {
                    'read': read_microseconds_tail_latency,
                    'update': update_microseconds_tail_latency,
                    'insert': insert_microseconds_tail_latency
                    }.items():
                    latency_by_record_count[record_count][op_proportions_key][op][percentile].append(latency)
                    latency_by_field_length[field_length][op_proportions_key][op][percentile].append(latency)
                    latency_by_field_count[field_count][op_proportions_key][op][percentile].append(latency)
                    latency_by_request_distribution[request_distribution][op_proportions_key][op][percentile].append(latency)

                    # Each of the above dicts should look something like, e.g. for latency_by_record_count
                    # {
                    #   1000: { # 1000 records
                    #       '0.9-0.1-0': { # ratio key
                    #           'read': {
                        #           '90': [...latencies...],
                        #           '95': [...latencies...],,
                        #           '99': [...latencies...],
                        #           '99.9': [...latencies...],
                        #           '99.99': [...latencies...],
                        #           },
                        #       'update': {
                        #           '90': [...latencies...],
                        #           '95': [...latencies...],,
                        #           '99': [...latencies...],
                        #           '99.9': [...latencies...],
                        #           '99.99': [...latencies...],
                        #           },
                        #        'insert': {
                        #           '90': [...latencies...],
                        #           '95': [...latencies...],,
                        #           '99': [...latencies...],
                        #           '99.9': [...latencies...],
                        #           '99.99': [...latencies...],
                    #           }
                    #       }
                    #   }
                    # }

        latencies = {
            'by_record_count': latency_by_record_count,
            'by_field_length': latency_by_field_length,
            'by_field_count': latency_by_field_count,
            'by_request_distribution': latency_by_request_distribution

        }
        return latencies

    def _plot_record_count_v_tail_latencies_for_operation(self, ycsb_stats=None, plot_folder='', operation='read', variable=""):
        """
        Plot a scatter plot showing relationship between [record_count, field_length, field_count, request_distribution]
         and tail latency for a specific operation [read, update, insert]
        x = variable [record_count, field_length, field_count, request_distribution]
        y = latency
        color of dots = percentile
        For the operation latency data, use the latency data where the proportion for that operation was 100%
        """
        data_key = f'by_{variable}'
        plot_file_name = f'{plot_folder}/{variable}-vs-tail-latency-{operation}.png'
        if variable == "record_count":
            pretty_name = f'Record Count'
        elif variable == "field_length":
            pretty_name = f'Field Length'
        elif variable == "field_count":
            pretty_name = f'Field Count'
        elif variable == "request_distribution":
            pretty_name = f'Request Distribution'
        else:
            self.error('You must use one of the following values for the variable parameter: [record_count, field_length, field_count, request_distribution]')

        if operation == "read":
            proportion_key = "1.0-0.0-0.0"
        elif operation == "update":
            proportion_key = "0.0-1.0-0.0"
        elif operation == "insert":
            proportion_key = "0.0-0.0-1.0"

        data = ycsb_stats[data_key]
        x_values = data.keys()
        self.info(f'X values: {x_values}')
        y_values_90 = []
        y_values_95 = []
        y_values_99 = []
        y_values_99_9 = []
        y_values_99_99 = []

        for key in x_values:
            y_values_90.append(data[key][proportion_key][operation]["90"])
            y_values_95.append(data[key][proportion_key][operation]["95"])
            y_values_99.append(data[key][proportion_key][operation]["99"])
            y_values_99_9.append(data[key][proportion_key][operation]["99.9"])
            y_values_99_99.append(data[key][proportion_key][operation]["99.99"])

        self.info(f"Y values length: {len(y_values_90)}")
        assert (y_values_90[0] != y_values_90[1])
        assert (y_values_95[0] != y_values_95[1])
        fig = plt.figure()
        ax = fig.add_subplot()
        ax.set_ylabel(u'Latency (\u03bcs)') # microseconds
        ax.set_xlabel(pretty_name)
        # plt.xticks([i for i in range(len(x_values))])
        # plt.axes().set_xticklabels([v for v in x_values])

        for xe, ye in zip(x_values, y_values_90):
            ax.scatter([xe] * len(ye), ye, color='b', label='90%')
        for xe, ye in zip(x_values, y_values_95):
            ax.scatter([xe] * len(ye), ye, color='g', label='95%')
        for xe, ye in zip(x_values, y_values_99):
            ax.scatter([xe] * len(ye), ye, color='r', label='99%')
        for xe, ye in zip(x_values, y_values_99_9):
            ax.scatter([xe] * len(ye), ye, color='c', label='99.9%')
        for xe, ye in zip(x_values, y_values_99_99):
            ax.scatter([xe] * len(ye), ye, color='m', label='99.99%')

        plt.tight_layout()
        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        plt.legend(by_label.values(), by_label.keys())


        plt.title(f'{pretty_name} vs. Tail Latency of {operation} in Couchbase')
        plt.savefig(plot_file_name)


    def plot_ycsb_stats(self, ycsb_stats=None):
        """ Take the YCSB stats generated by collect_ycsb_stats_to_json and generate plots """
        if not ycsb_stats:
            self.error("You need to provide ycsb_stats dictionary as argument")
            return None
        plot_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),'plots','ycsb')
        self.init_plot_folder(plot_folder)
        for op in ['read', 'update', 'insert']:
            for variable in ['record_count', 'field_length', 'field_count', 'request_distribution']:
                self._plot_record_count_v_tail_latencies_for_operation(
                    ycsb_stats=ycsb_stats,
                    plot_folder=plot_folder,
                    operation=op,
                    variable=variable)




if __name__ == "__main__":
    analyzer = Analyzer(verbose=True)
    ycsb_stats = analyzer.collect_ycsb_stats_to_json()
    # output_json_file = os.path.join(
    #         os.path.dirname(os.path.abspath(__file__)),
    #         'data','ycsb-stats.json')
    # with open(output_json_file, 'w') as f:
    #     json.dump(ycsb_stats , f)
    analyzer.plot_ycsb_stats(ycsb_stats=ycsb_stats)
